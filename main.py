import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score

from src.preprocessing import preparer_donnees, equilibrer_donnees
from src.vectorization import SimpleBagOfWords
from src.naive_bayes import MonNaiveBayes
from src.logistic_regression import MaRegressionLogistique
from src.visualization import (afficher_distribution_classes, afficher_matrice_confusion,
                                afficher_histogramme, comparer_modeles)
from sklearn_models import entrainer_modeles_pro

def afficher_metriques(nom, y_true, y_pred):
    """
    Affiche toutes les mÃ©triques importantes pour Ã©valuer un modÃ¨le.
    """
    acc = accuracy_score(y_true, y_pred)
    prec = precision_score(y_true, y_pred)
    rec = recall_score(y_true, y_pred)
    f1 = f1_score(y_true, y_pred)
    
    print(f"\n  ðŸ“ˆ RÃ©sultats : {nom}")
    print(f"     Accuracy  : {acc:.2%}  (Taux de bonnes prÃ©dictions)")
    print(f"     PrÃ©cision : {prec:.2%}  (Ã‰viter les faux positifs)")
    print(f"     Rappel    : {rec:.2%}  (DÃ©tecter tous les vrais toxiques)")
    print(f"     F1-Score  : {f1:.2%}  (Ã‰quilibre PrÃ©cision/Rappel)")
    
    return {'Acc': acc, 'Prec': prec, 'Rec': rec, 'F1': f1}

def discuter_resultats(resultats, y_test):
    """
    Ã‰TAPE 7 : DISCUSSION DES RÃ‰SULTATS
    Analyse comparative des performances et recommandations.
    """
    print("\n" + "="*70)
    print("ðŸ“Š DISCUSSION DES RÃ‰SULTATS".center(70))
    print("="*70)
    
    # 1. Comparaison Fait-Maison vs Scikit-Learn
    print("\nðŸ”¹ Comparaison : Nos ImplÃ©mentations vs Scikit-Learn")
    print("-" * 70)
    
    nb_diff = abs(resultats['NB (Maison)']['F1'] - resultats['NB (Sklearn)']['F1'])
    lr_diff = abs(resultats['LR (Maison)']['F1'] - resultats['LR (Sklearn)']['F1'])
    
    print(f"\n   Naive Bayes :")
    print(f"      Notre implÃ©mentation : {resultats['NB (Maison)']['F1']:.2%}")
    print(f"      Scikit-Learn         : {resultats['NB (Sklearn)']['F1']:.2%}")
    print(f"      DiffÃ©rence           : {nb_diff:.2%}")
    
    print(f"\n   RÃ©gression Logistique :")
    print(f"      Notre implÃ©mentation : {resultats['LR (Maison)']['F1']:.2%}")
    print(f"      Scikit-Learn         : {resultats['LR (Sklearn)']['F1']:.2%}")
    print(f"      DiffÃ©rence           : {lr_diff:.2%}")
    
    if nb_diff < 0.03 and lr_diff < 0.03:
        print("\n   âœ… Excellent ! Nos implÃ©mentations sont aussi bonnes que Scikit-Learn !")
    elif nb_diff < 0.05 and lr_diff < 0.05:
        print("\n   âœ… TrÃ¨s bien ! DiffÃ©rence acceptable (<5%)")
    else:
        print("\n   âš ï¸  Scikit-Learn performe mieux (optimisations internes)")
    
    # 2. Quel modÃ¨le est le meilleur ?
    print("\nðŸ”¹ Meilleur ModÃ¨le")
    print("-" * 70)
    
    meilleur = max(resultats.items(), key=lambda x: x[1]['F1'])
    print(f"\n   ðŸ† Champion : {meilleur[0]}")
    print(f"      F1-Score : {meilleur[1]['F1']:.2%}")
    
    if 'NB' in meilleur[0]:
        print("\n   ðŸ’¡ Naive Bayes gagne ! Pourquoi ?")
        print("      â€¢ HypothÃ¨se d'indÃ©pendance des mots fonctionne bien ici")
        print("      â€¢ Simple et efficace pour la classification de texte")
    else:
        print("\n   ðŸ’¡ RÃ©gression Logistique gagne ! Pourquoi ?")
        print("      â€¢ Capture mieux les relations entre mots")
        print("      â€¢ Plus flexible que Naive Bayes")
    
    # 3. Analyse de l'Ã©quilibre
    print("\nðŸ”¹ Ã‰quilibre du Jeu de Test")
    print("-" * 70)
    
    nb_toxiques = sum(y_test)
    nb_sains = len(y_test) - nb_toxiques
    ratio_tox = nb_toxiques / len(y_test)
    
    print(f"\n   Toxiques : {nb_toxiques} ({ratio_tox*100:.1f}%)")
    print(f"   Sains    : {nb_sains} ({(1-ratio_tox)*100:.1f}%)")
    
    if 0.4 <= ratio_tox <= 0.6:
        print("\n   âœ… Classes parfaitement Ã©quilibrÃ©es (40-60%) !")
        print("      â€¢ Les modÃ¨les apprennent Ã©quitablement")
        print("      â€¢ Accuracy et F1-Score sont fiables")
    else:
        print("\n   âš ï¸  DÃ©sÃ©quilibre dÃ©tectÃ©")
        print("      â€¢ PrivilÃ©gier le F1-Score plutÃ´t que l'Accuracy")
    
    # 4. Recommandations
    print("\nðŸ”¹ Pistes d'AmÃ©lioration")
    print("-" * 70)
    
    print("\n   ðŸš€ Pour aller plus loin :")
    print("      1. Utiliser TF-IDF au lieu de Bag of Words")
    print("      2. Augmenter le vocabulaire (2000-5000 mots)")
    print("      3. Tester des n-grams (bi-grams : 'trÃ¨s mauvais')")
    print("      4. Essayer d'autres modÃ¨les (SVM, Random Forest)")
    print("      5. Utiliser des embeddings (Word2Vec, BERT)")
    
    print("\n   ðŸ“š Concepts Ã  approfondir :")
    print("      â€¢ Validation croisÃ©e (K-Fold)")
    print("      â€¢ RÃ©gularisation (L1, L2)")
    print("      â€¢ Feature engineering avancÃ©")
    print("      â€¢ Deep Learning (LSTM, Transformers)")
    
    print("\n" + "="*70 + "\n")

def main():
    print("="*70)
    print(" PROJET ML : DÃ‰TECTION DE TOXICITÃ‰ ".center(70))
    print(" Version AcadÃ©mique ComplÃ¨te ".center(70))
    print("="*70 + "\n")

    # Ã‰TAPE 1 : CHARGEMENT DES DONNÃ‰ES
    print("ðŸ“‚ Ã‰TAPE 1 : Chargement des donnÃ©es...")
    try:
        df = pd.read_csv('data/archive/train.csv')
        df = df.sample(n=10000, random_state=42)
        print(f"   âœ… {len(df)} commentaires chargÃ©s")
    except FileNotFoundError:
        print("   âŒ Erreur : Fichier 'data/archive/train.csv' introuvable")
        return

    # Ã‰TAPE 2 : PRÃ‰TRAITEMENT + MISSING VALUES
    print("\nðŸ§¹ Ã‰TAPE 2 : PrÃ©traitement et gestion des valeurs manquantes...")
    df = preparer_donnees(df)
    print("   âœ… Textes nettoyÃ©s et valeurs manquantes traitÃ©es")

    # Ã‰TAPE 3 : ANALYSE EXPLORATOIRE (EDA)
    print("\nðŸ“Š Ã‰TAPE 3 : Analyse exploratoire (EDA)...")
    afficher_distribution_classes(df)
    print("   âœ… Graphique de distribution gÃ©nÃ©rÃ©")

    # Ã‰TAPE 4 : GESTION DU DÃ‰SÃ‰QUILIBRE
    print("\nâš–ï¸  Ã‰TAPE 4 : Gestion du dÃ©sÃ©quilibre des classes...")
    df = equilibrer_donnees(df)
    print(f"   âœ… Dataset Ã©quilibrÃ© : {len(df)} lignes")

    # Ã‰TAPE 5 : SÃ‰PARATION TRAIN/TEST
    print("\nâœ‚ï¸  Ã‰TAPE 5 : SÃ©paration Train/Test (80/20)...")
    X_train_brut, X_test_brut, y_train, y_test = train_test_split(
        df['texte_nettoye'], df['est_toxique'], test_size=0.2, random_state=42
    )
    print(f"   âœ… Train : {len(X_train_brut)} | Test : {len(X_test_brut)}")

    # Ã‰TAPE 6 : VECTORISATION + NORMALISATION
    print("\nðŸ”¢ Ã‰TAPE 6 : Vectorisation (Bag of Words) + Normalisation...")
    bow = SimpleBagOfWords(max_mots=1000, normaliser=True)
    X_train = bow.fit_transform(X_train_brut)
    X_test = bow.transform(X_test_brut)
    print("   âœ… Textes transformÃ©s en matrices numÃ©riques normalisÃ©es")

    # Ã‰TAPE 7 : ENTRAÃŽNEMENT DES MODÃˆLES
    print("\nðŸ¤– Ã‰TAPE 7 : EntraÃ®nement et Comparaison des ModÃ¨les")
    print("="*70)
    
    resultats = {}

    # --- Naive Bayes (Fait-Maison) ---
    print("\nðŸ”· Naive Bayes (ImplÃ©mentation Maison)")
    print("-" * 70)
    model_nb = MonNaiveBayes()
    model_nb.fit(X_train, y_train.values)
    pred_nb = model_nb.predict(X_test)
    scores_nb = model_nb.predict_proba(X_test)
    resultats['NB (Maison)'] = afficher_metriques("Naive Bayes (Maison)", y_test, pred_nb)
    afficher_matrice_confusion(y_test, pred_nb, "NaiveBayes_Maison")
    print("   âœ… Matrice de confusion sauvegardÃ©e")

    # --- RÃ©gression Logistique (Fait-Maison) ---
    print("\nðŸ”· RÃ©gression Logistique (ImplÃ©mentation Maison)")
    print("-" * 70)
    model_lr = MaRegressionLogistique(taux_apprentissage=0.01, iterations=200)
    model_lr.fit(X_train, y_train.values)
    pred_lr = model_lr.predict(X_test)
    scores_lr = model_lr.predict_proba(X_test)
    resultats['LR (Maison)'] = afficher_metriques("RÃ©gression Logistique (Maison)", y_test, pred_lr)
    afficher_matrice_confusion(y_test, pred_lr, "LogReg_Maison")
    print("   âœ… Matrice de confusion sauvegardÃ©e")

    # --- Comparaison avec Scikit-Learn ---
    print("\nðŸ”· ModÃ¨les Scikit-Learn (RÃ©fÃ©rence)")
    print("-" * 70)
    resultats_sklearn = entrainer_modeles_pro(X_train, y_train, X_test, y_test)
    
    resultats['NB (Sklearn)'] = {
        'Acc': resultats_sklearn['NB (Sklearn) Acc'],
        'F1': resultats_sklearn['NB (Sklearn) F1'],
        'Prec': 0,  # Non calculÃ© pour simplifier
        'Rec': 0
    }
    resultats['LR (Sklearn)'] = {
        'Acc': resultats_sklearn['LR (Sklearn) Acc'],
        'F1': resultats_sklearn['LR (Sklearn) F1'],
        'Prec': 0,
        'Rec': 0
    }
    
    print(f"\n  ðŸ“ˆ Naive Bayes (Sklearn)")
    print(f"     Accuracy : {resultats['NB (Sklearn)']['Acc']:.2%}")
    print(f"     F1-Score : {resultats['NB (Sklearn)']['F1']:.2%}")
    
    print(f"\n  ðŸ“ˆ RÃ©gression Logistique (Sklearn)")
    print(f"     Accuracy : {resultats['LR (Sklearn)']['Acc']:.2%}")
    print(f"     F1-Score : {resultats['LR (Sklearn)']['F1']:.2%}")

    # Ã‰TAPE 8 : VISUALISATIONS
    print("\nðŸ“Š Ã‰TAPE 8 : GÃ©nÃ©ration des Visualisations...")
    afficher_histogramme(scores_nb, "Scores_NaiveBayes")
    afficher_histogramme(scores_lr, "Scores_RegressionLogistique")
    
    # Graphique de comparaison (F1-Scores uniquement)
    f1_scores = {k: v['F1'] for k, v in resultats.items()}
    comparer_modeles(f1_scores)
    print("   âœ… Tous les graphiques gÃ©nÃ©rÃ©s")

    # Ã‰TAPE 9 : DISCUSSION DES RÃ‰SULTATS
    print("\nðŸ’¬ Ã‰TAPE 9 : Discussion et Analyse")
    discuter_resultats(resultats, y_test)

    # FIN
    print("="*70)
    print(" PROJET TERMINÃ‰ AVEC SUCCÃˆS ! ".center(70))
    print("="*70)
    print("\nâœ… Tous les fichiers gÃ©nÃ©rÃ©s :")
    print("   â€¢ eda_distribution.png")
    print("   â€¢ matrice_NaiveBayes_Maison.png")
    print("   â€¢ matrice_LogReg_Maison.png")
    print("   â€¢ scores_naivebayes.png")
    print("   â€¢ scores_regressionlogistique.png")
    print("   â€¢ comparaison_modeles.png")
    print("\nðŸ“š VÃ©rifiez les graphiques pour une analyse visuelle complÃ¨te !\n")

if __name__ == "__main__":
    main()